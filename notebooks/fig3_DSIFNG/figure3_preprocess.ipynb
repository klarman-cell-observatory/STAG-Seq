{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d93a4d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This script takes the Hypr-seq adata and the loom file as input\n",
    "# And output an scRNA-seq data with detailed annotations\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import loompy\n",
    "import logging\n",
    "import configparser\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path # For easier path handling\n",
    "import anndata as ad\n",
    "\n",
    "import copy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Read and filter the Hypr-seq anndata\n",
    "def filter_hypr(adata, probe_level=False):\n",
    "    # Remove cells with less than 1000 UMIs per cell\n",
    "    sc.pp.filter_cells(adata, min_counts=1000)\n",
    "    # merge the probe name, making the cell by probe matrix to cell by gene matrix\n",
    "    # Splitting at the underscore to separate gene names from probe numbers\n",
    "    if not probe_level:\n",
    "        gene_names = [name.split('_')[0] for name in adata.var_names]\n",
    "    else:\n",
    "        gene_names = [name for name in adata.var_names]\n",
    "\n",
    "    # Add the gene names as a new column in the DataFrame of the .var slot\n",
    "    adata.var['gene_name'] = gene_names\n",
    "    # Convert the AnnData to a DataFrame for easier manipulation\n",
    "    adata_df = pd.DataFrame(adata.X.T, index=adata.var_names, columns=adata.obs_names)\n",
    "\n",
    "    # Use the gene names to sum the counts\n",
    "    # Group by the new gene names and sum across columns (probes for the same gene)\n",
    "    aggregated_data = adata_df.groupby(adata.var['gene_name']).sum()\n",
    "\n",
    "    # Transpose back to original shape (samples as rows, genes as columns)\n",
    "    aggregated_data = aggregated_data.T\n",
    "\n",
    "    # Create new AnnData object with the aggregated data\n",
    "    adata_aggregated = sc.AnnData(X=aggregated_data)\n",
    "    # Copy the metadata from the original AnnData object\n",
    "    adata_aggregated.obs = adata.obs.copy()\n",
    "    # Optionally, copy over any relevant .uns data (unsupervised annotations, such as PCA, neighbors, etc.)\n",
    "    #adata_aggregated.uns = adata.uns.copy()\n",
    "\n",
    "    return adata_aggregated\n",
    "\n",
    "\n",
    "# function reads the loomfile downloaded from Tapestri portal\n",
    "def read_tapestri_loom(filename):\n",
    "    \"\"\"\n",
    "    Read data from MissionBio's formatted loom file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the loom file (.loom)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anndata.AnnData\n",
    "        An anndata object with the following layers:\n",
    "        adata.X: GATK calls\n",
    "        adata.layers['e']: reads with evidence of mutation\n",
    "        adata.layers['no_e']: reads without evidence of mutation\n",
    "    \"\"\"\n",
    "    loom_file = loompy.connect(filename)\n",
    "\n",
    "    variant_names, amplicon_names, chromosome, location = (\n",
    "        loom_file.ra['id'], loom_file.ra['amplicon'], loom_file.ra['CHROM'], loom_file.ra['POS'])\n",
    "\n",
    "    barcodes = [barcode.split('-')[0] for barcode in loom_file.ca['barcode']]\n",
    "    adata = anndata.AnnData(np.transpose(loom_file[:, :]), dtype=loom_file[:, :].dtype)\n",
    "    adata.layers['e'] = np.transpose(loom_file.layers['AD'][:, :])\n",
    "    adata.layers['no_e'] = np.transpose(loom_file.layers['RO'][:, :])\n",
    "    adata.var_names = variant_names\n",
    "    adata.obs_names = barcodes\n",
    "    adata.varm['amplicon'] = amplicon_names\n",
    "    adata.varm['chrom'] = chromosome\n",
    "    adata.varm['loc'] = location\n",
    "\n",
    "    loom_file.close()\n",
    "\n",
    "    return adata\n",
    "\n",
    "# Optional function, visualize the barcode distribution\n",
    "def barcode_rank_plot(adata, minimum=0, xmax=None):\n",
    "\n",
    "    # Sum the UMIs for each cell\n",
    "    cell_umi_counts_all = adata.X.sum(axis=1)\n",
    "    cell_idxs = np.argwhere(cell_umi_counts_all > minimum)\n",
    "    cell_umi_counts = cell_umi_counts_all[cell_idxs]\n",
    "\n",
    "    # Convert to numpy array if it's not already\n",
    "    cell_umi_counts = np.array(cell_umi_counts).flatten()\n",
    "\n",
    "    # Sort the UMI counts in descending order for the rank plot\n",
    "    sorted_umi_counts = np.sort(cell_umi_counts)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(5, 4), dpi=150)\n",
    "    sns.lineplot(x=range(1, len(sorted_umi_counts) + 1), y=sorted_umi_counts)\n",
    "    plt.xlabel('Barcode Rank')\n",
    "    plt.ylabel('UMI Count')\n",
    "    plt.title('Cell Barcode Rank Plot')\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "    plt.xscale('log') # Log scale\n",
    "    if xmax is not None:\n",
    "        plt.xlim(0, xmax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function for finding the intersecting barcodes between the modalities\n",
    "# We can drop the idea of mudata for simplity\n",
    "def find_intersecting_and_filter(adata_hypr, adata_loom):\n",
    "    \"\"\"\n",
    "    Find the intersecting barcode, reorder, and return new AnnData objects for both datasets.\n",
    "    \"\"\"\n",
    "    # Find intersecting barcodes\n",
    "    obs_1, obs_2 = adata_hypr.obs_names, adata_loom.obs_names\n",
    "    cmn_barcodes, idx_1, idx_2 = np.intersect1d(obs_1, obs_2, return_indices=True)\n",
    "\n",
    "    # Logging the number of barcodes\n",
    "    logger.info(f\"Found {len(obs_1)} barcodes in modality hypr seq\")\n",
    "    logger.info(f\"Found {len(obs_2)} barcodes in modality loom file\")\n",
    "    logger.info(f\"Found {len(idx_1)} intersecting barcodes\")\n",
    "\n",
    "    # Subset and reorder both datasets based on the intersecting indices\n",
    "    adata_hypr_new = adata_hypr[idx_1, :]\n",
    "    adata_loom_new = adata_loom[idx_2, :]\n",
    "\n",
    "    # # Ensure the order of barcodes is the same in both datasets\n",
    "    # adata_hypr_new = adata_hypr_new[cmn_barcodes, :]\n",
    "    # adata_loom_new = adata_loom_new[cmn_barcodes, :]\n",
    "\n",
    "    return adata_hypr_new, adata_loom_new\n",
    "    \n",
    "\n",
    "# Step 1, determine the germline mutation from the loom file\n",
    "# We envision that if certain mutation appeared too many times in the dataset\n",
    "# (e.g., more than 25% cells have the homo mutation, we call it homo germline)\n",
    "def call_germline_mutations(adata_loom, cutoff=0.25):\n",
    "\n",
    "    fraction_het = np.mean(adata_loom.X == 1, axis=0)\n",
    "    het_germline = adata_loom.var_names[fraction_het > cutoff]\n",
    "\n",
    "    fraction_hom = np.mean(adata_loom.X == 2, axis=0)\n",
    "    hom_germline = adata_loom.var_names[fraction_hom > cutoff]\n",
    "\n",
    "    return list(het_germline) + list(hom_germline)\n",
    "\n",
    "\n",
    "# For AAV control, we may ignore mixed cells and bystander effect\n",
    "def call_AAV_control_edits(adata_loom, config_path='AAV_config.ini'):\n",
    "    \"\"\"\n",
    "    Input: Tapestri loom file and mutation configuration \n",
    "    Current we only accept 1 type of AAV control (should be a continuous region)\n",
    "    Output: All the AAV edits found in the loom file. \n",
    "\n",
    "    This function will not modify adata_loom\n",
    "    \"\"\"\n",
    "    # Read configuration settings\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    chrom_name = config.get('AAV_control', 'chrom_name')\n",
    "    start = config.getint('AAV_control', 'start')\n",
    "    end = config.getint('AAV_control', 'end')\n",
    "    variant_alleles = config.get('AAV_control', 'variant_alleles')\n",
    "    \n",
    "    # Parsing variant names\n",
    "    variant_names = np.asarray(adata_loom.var_names.values)\n",
    "    chrom = [name.split(':')[0] for name in variant_names]\n",
    "    loc = [int(name.split(':')[1]) for name in variant_names]\n",
    "    edit_type = [name.split(':')[2] for name in variant_names]\n",
    "\n",
    "    control_editing = []\n",
    "    complement_rule = {'C': 'G', 'G': 'C', 'A': 'T', 'T': 'A'}\n",
    "    try:\n",
    "        reverse_variant_alleles = complement_rule[variant_alleles[0]] + \"/\" + complement_rule[variant_alleles[-1]]\n",
    "    except:\n",
    "        raise ValueError(f\"The variant alleles is not supported. Current variant alleles is {variant_alleles}\")\n",
    "    # Identify control editing cells\n",
    "    for i in range(len(chrom)):\n",
    "        if chrom[i] == chrom_name and start <= loc[i] < end+1:\n",
    "            if edit_type[i] in [variant_alleles, reverse_variant_alleles]:\n",
    "                control_editing.append(variant_names[i])\n",
    "\n",
    "    return control_editing\n",
    "\n",
    "\n",
    "def call_AAV_del_edits(adata_loom, config_path):\n",
    "    \"\"\"\n",
    "    Input: Tapestri loom file and mutation configuration \n",
    "    Current we only accept 1 type of AAV control (should be a continuous region)\n",
    "    Output: All the AAV edits found in the loom file. \n",
    "    \"\"\"\n",
    "    # Read configuration settings\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    chrom_name = config.get('AAV_del', 'chrom_name')\n",
    "    start = config.getint('AAV_del', 'start')\n",
    "    end = config.getint('AAV_del', 'end')\n",
    "    \n",
    "    # Parsing variant names\n",
    "    variant_names = np.asarray(adata_loom.var_names.values)\n",
    "    chrom = [name.split(':')[0] for name in variant_names]\n",
    "    loc = [int(name.split(':')[1]) for name in variant_names]\n",
    "    edit_type = [name.split(':')[2] for name in variant_names]\n",
    "\n",
    "    del_editing = []\n",
    "    \n",
    "    # Identify  editing cells\n",
    "    for i in range(len(chrom)):\n",
    "        if chrom[i] == chrom_name and start <= loc[i] < end+1:\n",
    "            if edit_type[i][-1] == \"*\":\n",
    "                del_editing.append(variant_names[i])\n",
    "\n",
    "    return del_editing\n",
    "\n",
    "\n",
    "def call_AAV_cells(adata_hypr, adata_loom, AAV_editing):\n",
    "    \"\"\"\n",
    "    Annotate the hypr adata (scRNA-seq data) by the AAV_edit found in call_AAV_edits.\n",
    "    We do not perform annotation in the function to ensure consistency\n",
    "    \"\"\"\n",
    "    all_idx = []\n",
    "    for edit in AAV_editing:\n",
    "        # get the idx of the edit we found\n",
    "        idx = adata_loom.var_names.get_loc(edit)\n",
    "        # if the value is 1/2 (hete/homo), we annotate the cell as control edit cell\n",
    "        het_idx, hom_idx = [np.flatnonzero(adata_loom.X[:, idx] == i) for i in (1, 2)]\n",
    "        AAV_edit_idx = np.concatenate([het_idx, hom_idx])\n",
    "        AAV_cells_barcode_loom = adata_loom.obs_names[AAV_edit_idx]\n",
    "        # find the AAV cells index in that of the adata_hypr\n",
    "        _, idx1, _ = np.intersect1d(adata_hypr.obs_names, AAV_cells_barcode_loom, return_indices=True)\n",
    "        all_idx.append(idx1)\n",
    "        \n",
    "    return np.unique(np.concatenate(all_idx))\n",
    "\n",
    "\n",
    "\n",
    "def get_nearby_variants(\n",
    "    variant_names, # List of all variant in the loom file\n",
    "    target_loc, # loci of interest\n",
    "    germline_amplicons, # List of Germline mutations \n",
    "    window_size # window size\n",
    "):\n",
    "    \"\"\"\n",
    "        Get nearby variants within a window size of the target_loc.\n",
    "        Args:\n",
    "            variant_names (list): List of variant names.\n",
    "            target_loc (str): Target loci (e.g. chr1:123456:A/G).\n",
    "            germline_amplicons (list): List of germline amplicons.\n",
    "            window_size (int): Window size.\n",
    "        Returns:\n",
    "            list: List of nearby variants.\n",
    "    \"\"\"\n",
    "    chrom_t, loc_t, _ = target_loc.split(\":\")\n",
    "    nearby_variants = []\n",
    "    for variant in variant_names:\n",
    "        chrom, loc, edit_type = variant.split(\":\")\n",
    "        is_valid_edit_type = not edit_type.endswith(\"*\") and re.match(\n",
    "            r\"^[A-Za-z]/[A-Za-z]$\", edit_type\n",
    "        )\n",
    "        is_not_germline = variant not in germline_amplicons\n",
    "        if (\n",
    "            chrom == chrom_t\n",
    "            and (int(loc_t) - window_size) <= int(loc) <= (int(loc_t) + window_size)\n",
    "            and is_valid_edit_type\n",
    "            and is_not_germline\n",
    "        ):\n",
    "            nearby_variants.append(variant)\n",
    "\n",
    "    return nearby_variants\n",
    "\n",
    "\n",
    "\n",
    "def plot_mutant_types(target_loc, new_matrix, mt_type, snp_name, save_path, index=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the nearby genotype heatmap\n",
    "    \"\"\"\n",
    "    mutation_counts = new_matrix.apply(np.count_nonzero, axis=0)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(mutation_counts.index, mutation_counts.values)\n",
    "    for i, idx in enumerate(mutation_counts.index):\n",
    "        plt.text(\n",
    "            bars[i].get_x() + bars[i].get_width() / 2,\n",
    "            bars[i].get_height(),\n",
    "            str(mutation_counts.values[i]),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "    if target_loc in mutation_counts.index:\n",
    "        bars[mutation_counts.index.tolist().index(target_loc)].set_color(\"r\")\n",
    "    red_patch = mpatches.Patch(color=\"red\", label=\"Target Loci\")\n",
    "    plt.legend(handles=[red_patch])\n",
    "\n",
    "    # we need to rename the target_loc to ensure we do not introduce extra path problems\n",
    "    target_loc = target_loc.replace(\"/\", \"-\")\n",
    "\n",
    "\n",
    "    plot_title = (\n",
    "        f\"Mutation Counts: {mt_type} - {target_loc} ({index}) with total cells {len(new_matrix)}\"\n",
    "    )\n",
    "    plt.title(plot_title)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"Number of Mutated Cells\")\n",
    "\n",
    "    save_index = f\"{snp_name}_{mt_type}_mutant_counts_{index}.pdf\"\n",
    "    plt.savefig(os.path.join(save_path, save_index), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot a cluster map of matrix but cluster rows only.\n",
    "    sns.clustermap(\n",
    "        new_matrix,\n",
    "        cmap=\"YlGnBu\",\n",
    "        row_cluster=True,\n",
    "        col_cluster=False,\n",
    "        figsize=(10, 10),\n",
    "    )\n",
    "    plt.title(\n",
    "        f\"Mutation Matrix: {mt_type} - {snp_name} aka {target_loc} with shape {new_matrix.shape}\"\n",
    "    )\n",
    "    plt.xlabel(\"Variant\")\n",
    "    plt.ylabel(\"Cell Barcode\")\n",
    "    save_index = f\"{snp_name}_{mt_type}_mutant_matrix_{index}.pdf\"\n",
    "    plt.savefig(os.path.join(save_path, save_index), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_bystander_cells(target_loc, cells, nearby_variants, adata_loom):\n",
    "    \"\"\"\n",
    "    Identify which cells have bystander editing.\n",
    "    Args:\n",
    "        target_loc (str): Target loci (e.g. chr1:123456:A/G).\n",
    "        cells (list): List of cells (that supposedly have the mutation).\n",
    "        nearby_variant: adata that includes only the nearby cells\n",
    "    Returns:\n",
    "        list: List of pure cells.\n",
    "        pd.DataFrame: New matrix.\n",
    "        pd.DataFrame: Matrix with false amplicons.\n",
    "    \"\"\"\n",
    "    nearby_adata = adata_loom[cells, nearby_variants].copy()\n",
    "    new_matrix = pd.DataFrame(\n",
    "        nearby_adata.X, index=nearby_adata.obs_names, columns=nearby_adata.var_names\n",
    "    )\n",
    "    # Change all 3 to 0 in the new matrix, but excluding the target loci column.\n",
    "    for col in new_matrix.columns:\n",
    "        if col != target_loc:\n",
    "            new_matrix[col] = new_matrix[col].apply(\n",
    "                lambda val: 0 if val == 3 else val\n",
    "            )\n",
    "\n",
    "    # Remove the target loci column to generate a bystander position only matrix.\n",
    "    matrix_without_target_loci = new_matrix.loc[\n",
    "        :, new_matrix.columns != target_loc\n",
    "    ].copy()\n",
    "    bystander_sum_for_every_cell = matrix_without_target_loci.sum(axis=1)\n",
    "    # Remove cells that have a row sum of 0, or there's no bystander editing.\n",
    "    matrix_without_target_loci = matrix_without_target_loci.loc[\n",
    "        bystander_sum_for_every_cell != 0\n",
    "    ]\n",
    "\n",
    "    # If there's no bystander editing, return the original cells.\n",
    "    if len(matrix_without_target_loci) == 0:\n",
    "        # Set the bystander as the empty set\n",
    "        return set(cells), set(), new_matrix\n",
    "\n",
    "    # Everything that's left is a bystander cell.\n",
    "    pure_cells = set(cells) - set(matrix_without_target_loci.index)\n",
    "    return pure_cells, set(matrix_without_target_loci.index), new_matrix\n",
    "\n",
    "\n",
    "\n",
    "def call_single_loci_cells(\n",
    "    target_loc, # the loci of interest\n",
    "    adata_loom, # the loom matrix to retrieve info\n",
    "    window_size, # the window size \n",
    "    germline_amplicons, # The germline mutations\n",
    "    snp_name, # the name of the loci\n",
    "    plot_path, # the path to save the figures\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single target loci. \n",
    "    We will annotate all cells that include the loci\n",
    "    The values represent the mutant type:\n",
    "        0: Background\n",
    "        1: Pure heterozygous\n",
    "        2: Heterozygous with bystander editing\n",
    "        3: Pure homozygous\n",
    "        4: Homozygous with bystander editing\n",
    "    \"\"\"\n",
    "    # Find the idx of the loci\n",
    "    idx = adata_loom.var_names.get_loc(target_loc)\n",
    "    # Annotate cells that are background, het, and hom based on the loom matrix\n",
    "    bkg_cells, het_cells, hom_cells = [\n",
    "            adata_loom.obs_names[np.flatnonzero(adata_loom.X[:, idx] == i)] for i in (0, 1, 2)\n",
    "            ]\n",
    "    # Find all nearby variant of the give loci\n",
    "    variant_names = adata_loom.var_names\n",
    "    nearby_variants = get_nearby_variants(\n",
    "            variant_names, target_loc, germline_amplicons, window_size\n",
    "    )\n",
    "\n",
    "    # ignore synonymous mutations\n",
    "    if target_loc == \"chr2:190986868:C/T\":\n",
    "        if 'chr2:190986872:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190986872:C/T\")\n",
    "\n",
    "    if target_loc == \"chr2:190986872:C/T\":\n",
    "        if 'chr2:190986868:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190986868:C/T\")\n",
    "\n",
    "    if target_loc == \"chr2:190997872:C/T\":\n",
    "        if 'chr2:190997873:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190997873:C/T\")\n",
    "    \n",
    "    if target_loc == \"chr2:190997873:C/T\":\n",
    "        if 'chr2:190997872:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190997872:C/T\")\n",
    "    \n",
    "    if target_loc == \"chr9:5054789:G/A\":\n",
    "        if 'chr9:5054790:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr9:5054790:G/A\")\n",
    "\n",
    "    if target_loc == \"chr9:5054790:G/A\":\n",
    "        if 'chr9:5054789:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr9:5054789:G/A\")\n",
    "    \n",
    "    if target_loc == \"chr21:33421686:G/A\":\n",
    "        if 'chr21:33421679:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr21:33421679:C/T\")\n",
    "    \n",
    "    if target_loc == \"chr21:33421679:C/T\":\n",
    "        if 'chr21:33421686:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr21:33421686:G/A\")\n",
    "\n",
    "\n",
    "    # the following ignore nearby variants are to remove synonymous mutations\n",
    "    if target_loc == \"chr1:64864885:G/A\":\n",
    "        if 'chr1:64864886:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr1:64864886:G/A\")\n",
    "    \n",
    "    if target_loc == \"chr1:64873414:T/C\":\n",
    "        if 'chr1:64873415:T/C' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr1:64873415:T/C\")\n",
    "\n",
    "    if target_loc == \"chr1:64879107:C/T\":\n",
    "        if 'chr1:64879105:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr1:64879105:C/T\")\n",
    "    \n",
    "    if target_loc == \"chr2:190995142:G/A\":\n",
    "        if 'chr2:190995144:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190995144:G/A\")\n",
    "        if 'chr2:190995141:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190995141:G/A\")\n",
    "\n",
    "    if target_loc == \"chr2:190999673:T/C\":\n",
    "        if 'chr2:190999675:T/C' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190999675:T/C\")\n",
    "        if 'chr2:190999669:T/C' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr2:190999669:T/C\")\n",
    "\n",
    "    if target_loc == \"chr5:132486824:G/A\":\n",
    "        if 'chr5:132486825:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr5:132486825:G/A\")\n",
    "\n",
    "    if target_loc == \"chr6:137206214:A/G\":\n",
    "        if 'chr6:137206215:A/G' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr6:137206215:A/G\")\n",
    "\n",
    "    if target_loc == \"chr6:137206249:A/G\":\n",
    "        if 'chr6:137206248:A/G' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr6:137206248:A/G\")\n",
    "        if 'chr6:137206251:A/G' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr6:137206251:A/G\")\n",
    "\n",
    "    if target_loc == \"chr9:5078360:A/G\":\n",
    "        if 'chr9:5078362:A/G' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr9:5078362:A/G\")\n",
    "\n",
    "    if target_loc == \"chr9:5089702:G/A\":\n",
    "        if 'chr9:5089703:G/A' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr9:5089703:G/A\")\n",
    "\n",
    "    if target_loc == \"chr21:33421679:C/T\":\n",
    "        if 'chr21:33421675:C/T' in nearby_variants:\n",
    "            nearby_variants.remove(\"chr21:33421675:C/T\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    het_pure, het_bystander, het_matrix = process_bystander_cells(\n",
    "            target_loc, het_cells, nearby_variants, adata_loom\n",
    "    )\n",
    "    hom_pure, hom_bystander, hom_matrix = process_bystander_cells(\n",
    "            target_loc, hom_cells, nearby_variants, adata_loom\n",
    "    )\n",
    "    # het_matrix.fillna(0)\n",
    "    # hom_matrix.fillna(0)\n",
    "    try:\n",
    "        if len(list(het_bystander))>1:\n",
    "            plot_mutant_types(target_loc, het_matrix.fillna(0), \"Heterozygous\", snp_name, plot_path, \"1-0\")\n",
    "        if len(list(hom_bystander))>1:\n",
    "            plot_mutant_types(target_loc, hom_matrix.fillna(0), \"Homozygous\", snp_name, plot_path, \"1-0\")\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "        raise ValueError(\"wrong figure plotting again\")\n",
    "\n",
    "    return list(het_bystander), list(hom_bystander), list(het_pure), list(hom_pure)\n",
    "\n",
    "    # adata_hypr.loc[hom_pure, \"genotype\"] = f\"{target_loc}_homo_pure\"\n",
    "    # adata_hypr.loc[het_pure, \"genotype\"] = f\"{target_loc}_hete_pure\"\n",
    "\n",
    "\n",
    "def annotate_cells(adata):\n",
    "    \"\"\"\n",
    "    Annotates each cell in adata.obs based on the SNP, control, and del columns, according to specified rules.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object containing the obs DataFrame with binary columns for each condition.\n",
    "    \n",
    "    Returns:\n",
    "    - Annotations are stored in a new column 'annotation' in adata.obs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of condition columns, based on the presence of SNPs, control, and del columns in adata.obs\n",
    "    snp_columns = [col for col in adata.obs.columns if any(sub in col for sub in [\"het_bystander\", \"het_pure\", \"hom_bystander\", \"hom_pure\"])]\n",
    "    control_column = \"AAV_control\"\n",
    "    # del_column = \"AAV_del\"\n",
    "    \n",
    "    # Define the function for annotating a single cell\n",
    "    def annotate_row(row):\n",
    "        # Check SNP-related rules\n",
    "        snp_active = row[snp_columns].sum()  # Count number of active SNP columns (True values)\n",
    "        \n",
    "        if snp_active > 1:  # More than one SNP column is True\n",
    "            return \"mixed\"\n",
    "        \n",
    "        if row[control_column] and snp_active == 1:  # SNP and control active\n",
    "            # Find which SNP column is active\n",
    "            snp_name = row[snp_columns][row[snp_columns] == True].index[0]\n",
    "            return \"mixed\"\n",
    "        \n",
    "        \n",
    "        if row[control_column] and snp_active == 0:  # Only control active\n",
    "            return \"AAV_control\"\n",
    "        \n",
    "        \n",
    "        if snp_active == 1:  # Single SNP active, no control or del\n",
    "            snp_name = row[snp_columns][row[snp_columns] == True].index[0]\n",
    "            return snp_name\n",
    "        \n",
    "        if snp_active == 0 and not row[control_column]:\n",
    "            return \"unedited\"\n",
    "        \n",
    "        # Default case, though we should not reach here\n",
    "        return \"unknown\"\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # Apply the annotation function to each row in a vectorized manner\n",
    "    adata.obs['genotype_annotation'] = adata.obs.apply(annotate_row, axis=1)\n",
    "\n",
    "\n",
    "def annotate_cells_v2(adata):\n",
    "    \"\"\"\n",
    "    Annotates each cell in adata.obs based on SNP columns, applying special rules for a \n",
    "    predefined set of \"bad amplicon\" genotypes.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing the obs DataFrame with binary columns for each genotype.\n",
    "\n",
    "    Returns:\n",
    "    - Annotations are stored in a new column 'genotype_annotation' in adata.obs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your set of \"bad amplicon\" genotypes here\n",
    "    # These are the SNPs that will follow the new rules.\n",
    "    bad_amplicon_genotype = [\n",
    "        'STAT1-A402T', \n",
    "        'IRF1-W11R',\n",
    "        'IFNGR1-E164K',\n",
    "        'IFNGR1-NC2',\n",
    "        'JAK2-G281S',\n",
    "        'JAK2-G281D'\n",
    "    ]\n",
    "    # add het_pure, home_pure, het_bystander, hom_bystander\n",
    "    bad_amplicon_genotype = [f\"{snp}_{suffix}\" for snp in bad_amplicon_genotype for suffix in [\"het_pure\", \"hom_pure\", \"het_bystander\", \"hom_bystander\"]]\n",
    "\n",
    "    # Identify all SNP-related columns from your AnnData object\n",
    "    snp_columns = [col for col in adata.obs.columns if any(sub in col for sub in [\"het_bystander\", \"het_pure\", \"hom_bystander\", \"hom_pure\"])]\n",
    "    control_column = \"AAV_control\"\n",
    "\n",
    "    # Separate the SNP columns into \"bad\" and \"good\" for easier processing\n",
    "    bad_snp_columns = [col for col in snp_columns if col in bad_amplicon_genotype]\n",
    "    good_snp_columns = [col for col in snp_columns if col not in bad_amplicon_genotype]\n",
    "\n",
    "    def annotate_row(row):\n",
    "        # Determine the state of different SNP types and controls\n",
    "        is_aav_active = row[control_column]\n",
    "        active_bad_snps = row[bad_snp_columns][row[bad_snp_columns] == True].index.tolist()\n",
    "        active_good_snps = row[good_snp_columns][row[good_snp_columns] == True].index.tolist()\n",
    "        \n",
    "        num_active_bad = len(active_bad_snps)\n",
    "        num_active_good = len(active_good_snps)\n",
    "\n",
    "        # --- Rule Set 1: Apply special logic if any \"bad amplicon\" SNP is active ---\n",
    "        if num_active_bad > 0:\n",
    "            \n",
    "            # Rule 2: If a bad SNP and one other \"good\" SNP are active, annotate as the \"good\" SNP.\n",
    "            if num_active_good == 1:\n",
    "                return active_good_snps[0]\n",
    "                \n",
    "            # Rule 3: If AAV_control is also active, call it AAV_control.\n",
    "            if is_aav_active:\n",
    "                return \"AAV_control\"\n",
    "            \n",
    "            # Rule 1: If only one SNP is active and it's a \"bad\" one, annotate as that bad SNP.\n",
    "            if num_active_good == 0 and num_active_bad == 1:\n",
    "                return active_bad_snps[0]\n",
    "            \n",
    "            # Edge Case: If a bad SNP and multiple \"good\" SNPs are active, or multiple bad SNPs are active.\n",
    "            # This defaults to \"mixed\" as the outcome is ambiguous.\n",
    "            return \"mixed\"\n",
    "\n",
    "        # --- Rule Set 2: Fallback to original logic if NO \"bad amplicon\" SNPs are active ---\n",
    "        else: # num_active_bad == 0\n",
    "            # More than one \"good\" SNP is active\n",
    "            if num_active_good > 1:\n",
    "                return \"mixed\"\n",
    "            \n",
    "            # One \"good\" SNP is active\n",
    "            if num_active_good == 1:\n",
    "                # # If AAV_control is also active, it's mixed\n",
    "                # if is_aav_active:\n",
    "                #     return \"mixed\"\n",
    "                # # Otherwise, it's just the SNP itself\n",
    "                # else:\n",
    "                return active_good_snps[0]\n",
    "            \n",
    "            # No \"good\" SNPs are active (num_active_good == 0)\n",
    "            if num_active_good == 0:\n",
    "                if is_aav_active:\n",
    "                    return \"AAV_control\"\n",
    "                else:\n",
    "                    return \"unedited\"\n",
    "        \n",
    "        # Default fallback, should not be reached with the logic above\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Apply the annotation function to each row\n",
    "    adata.obs['genotype_annotation'] = adata.obs.apply(annotate_row, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def annotate_genotype(adata_loom, \n",
    "                    adata_hypr, \n",
    "                    config_path,\n",
    "                    save_path=\"./\",\n",
    "                    plot_path=\"./\"):\n",
    "    \"\"\"\n",
    "    Annotate the adata_hypr, such that in the adata_hypr.obs, we have:\n",
    "    Homo pure for SNP 1, 2, 3, …\n",
    "    Homo bystander for SNP 1,2, 3, …\n",
    "    Hete pure for SNP 1, 2, 3, …\n",
    "    Hete bystander for SNP 1, 2, 3…\n",
    "    Mixed cells\n",
    "    AAVs: AAV_control, AAV_del\n",
    "    Unedited. \n",
    "    \"\"\"\n",
    "\n",
    "    # Perform the annotation step. We need:\n",
    "    # 1. Determine the germline mutations\n",
    "    # 2. Annotate the AAV_control cells\n",
    "    # 3. Annotate the AAV_del_control cells\n",
    "    # 4. For each SNP we are interested in, find if it has bystander effect\n",
    "    # 5. Determine the bystander mutations\n",
    "    # 6.  Annotate all mixed cells. For example, for 2 SNPs we are interested in, \n",
    "    #  if they are co-edited, we need to count the number. We need to annotate it as mixed.\n",
    "    \n",
    "\n",
    "    # Step 1, determine the germline mutations\n",
    "    germline_amplicons = call_germline_mutations(adata_loom, cutoff=0.25)\n",
    "\n",
    "    # Step 2, annotate the AAV_control cells\n",
    "    # 2.1 Find the control editing locis \n",
    "    AAV_control_editing = call_AAV_control_edits(adata_loom, config_path=config_path)\n",
    "    # 2.2 Add a column to the adata_hypr obs and then perform the annotation only here\n",
    "    adata_hypr.obs[\"AAV_control\"] = False\n",
    "    AAV_control_cell_idx = call_AAV_cells(adata_hypr, adata_loom, AAV_editing=AAV_control_editing)\n",
    "    adata_hypr.obs.iloc[AAV_control_cell_idx, adata_hypr.obs.columns.get_loc(\"AAV_control\")] = True\n",
    "\n",
    "    # Step 3, annotate the AAV_del_control_cells\n",
    "    # 3.1 Find the del editing locis\n",
    "    try:\n",
    "        AAV_del_editing = call_AAV_del_edits(adata_loom, config_path=config_path)\n",
    "        # 3.2 Add a column to the adata_hypr and then perform the annotation only here\n",
    "        adata_hypr.obs[\"AAV_del\"] = False\n",
    "        AAV_del_cell_idx = call_AAV_cells(adata_hypr, adata_loom, AAV_editing=AAV_del_editing)\n",
    "        adata_hypr.obs.iloc[AAV_del_cell_idx, adata_hypr.obs.columns.get_loc(\"AAV_del\")] = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Step 4/5, Enumerate all other variants we are interested in\n",
    "    # Initialize the config parser\n",
    "    config = configparser.ConfigParser()\n",
    "    # Read the config file\n",
    "    config.read(config_path)\n",
    "    # Retrieve and return all section names\n",
    "    sections = config.sections()\n",
    "    exclude_sections = [\"AAV_control\", \"AAV_del\"]\n",
    "    SNPs = [section for section in sections if section not in exclude_sections]\n",
    "\n",
    "    # Each snp is the section name we defined in the config file\n",
    "    # Perform the annotation here\n",
    "    annotation = [\"het_bystander\", \"het_pure\", \"hom_bystander\", \"hom_pure\"]\n",
    "\n",
    "    for snp in SNPs:\n",
    "        chrom_name = config.get(snp, 'chrom_name')\n",
    "        locus = config.getint(snp, 'locus')\n",
    "        alleles = config.get(snp, 'variant_alleles')\n",
    "        complement_rule = {'C': 'G', 'G': 'C', 'A': 'T', 'T': 'A'}\n",
    "        try:\n",
    "            complement_variant_alleles = complement_rule[alleles[0]] + \"/\" + complement_rule[alleles[-1]]\n",
    "        except:\n",
    "            raise ValueError(f\"The variant alleles is not supported. Current variant alleles is {alleles}\")\n",
    "        # Transfer back to the original version\n",
    "        target_loc = \":\".join([chrom_name, str(locus), alleles])\n",
    "        complement_target_loc = \":\".join([chrom_name, str(locus), complement_variant_alleles])\n",
    "        \n",
    "        # Find which allele is the one we want, the original version or the complement one\n",
    "        if target_loc in adata_loom.var_names:\n",
    "            target_loc = target_loc\n",
    "        elif complement_target_loc in adata_loom.var_names:\n",
    "            target_loc = complement_target_loc\n",
    "        else:\n",
    "            raise ValueError(f\"Both the mutation {target_loc} and the reversed one {complement_target_loc} is not found in the loom file\")\n",
    "\n",
    "        # if snp == \"JAK2-R683G\":\n",
    "        #     import pdb; pdb.set_trace()\n",
    "        # Get the index of the cells\n",
    "        het_bystander, hom_bystander, het_pure, hom_pure = call_single_loci_cells(target_loc, adata_loom, window_size=10, \n",
    "            germline_amplicons=germline_amplicons, snp_name = snp, plot_path=plot_path)\n",
    "        \n",
    "\n",
    "        for i, idx_i in enumerate([het_bystander, het_pure, hom_bystander, hom_pure]):\n",
    "            adata_hypr.obs[f\"{snp}_{annotation[i]}\"] = False\n",
    "            adata_hypr.obs.loc[idx_i, f\"{snp}_{annotation[i]}\"] = True\n",
    "\n",
    "        # adata_hypr.obs[f\"{snp}\"] = adata_hypr.obs[f\"{snp}_het_all\"] | adata_hypr.obs[f\"{snp}_hom_all\"]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # Step 7, for the cells that has genotypes, we need to ensure that they are not mixed cells\n",
    "    annotate_cells_v2(adata_hypr)\n",
    "\n",
    "    return adata_hypr, adata_loom\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description=\"Filter, Intersect, Merge, and Annotate genotype and phenotype data from multiple experiments.\")\n",
    "\n",
    "    # --- Define Input Datasets ---\n",
    "    # This structure defines the paths and metadata for your 5 datasets.\n",
    "    # MODIFY THIS SECTION with your actual file paths and desired batch names.\n",
    "    # Assumes a base directory structure, adjust as needed.\n",
    "    # You could also load this from a metadata CSV file.\n",
    "    datasets = {\n",
    "        'ifng_rep1': {\n",
    "            'loom_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/tapestri_loom/v3/MDM_DSIFNG_IFNg_gDNA1_v3.cells.loom',\n",
    "            'hypr_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/hypr_probe_matrix/MDM_DSIFNG_IFNg_HyPR1.txt', # Or other hypr format\n",
    "            'condition': 'IFNG',\n",
    "            'batch': 'rep1',\n",
    "        },\n",
    "        'ifng_rep2': {\n",
    "            'loom_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/tapestri_loom/v3/MDM_DSIFNG_IFNg_gDNA2_v3.cells.loom',\n",
    "            'hypr_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/hypr_probe_matrix/MDM_DSIFNG_IFNg_HyPR2.txt',\n",
    "            'condition': 'IFNG',\n",
    "            'batch': 'rep2',\n",
    "        },\n",
    "        'ifng_rep3': {\n",
    "            'loom_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/tapestri_loom/v3/MDM_DSIFNG_IFNg_gDNA3_v3.cells.loom',\n",
    "            'hypr_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/hypr_probe_matrix/MDM_DSIFNG_IFNg_HyPR3.txt',\n",
    "            'condition': 'IFNG',\n",
    "            'batch': 'rep3',\n",
    "        },\n",
    "        'control_rep1': {\n",
    "            'loom_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/tapestri_loom/v3/MDM_DSIFNG_Ctrl_gDNA1_v3.cells.loom',\n",
    "            'hypr_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/hypr_probe_matrix/MDM_DSIFNG_Ctrl_HyPR1.txt',\n",
    "            'condition': 'PBS',\n",
    "            'batch': 'rep4', # Note: Batch names can overlap if conditions differ\n",
    "        },\n",
    "        'control_rep2': {\n",
    "            'loom_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/tapestri_loom/v3/MDM_DSIFNG_Ctrl_gDNA2_v3.cells.loom',\n",
    "            'hypr_path': '/mnt/data/project/25_04_29_Figure3_reanalysis/data_raw/hypr_probe_matrix/MDM_DSIFNG_Ctrl_HyPR2.txt',\n",
    "            'condition': 'PBS',\n",
    "            'batch': 'rep5',\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add arguments for common paths and settings\n",
    "    parser.add_argument('--config_path', type=str, required=True, help='The path to the variant configuration file (INI format)')\n",
    "    parser.add_argument('--save_path', type=str, required=True, help='The base path to save annotated anndata files')\n",
    "    parser.add_argument('--plot_path', type=str, required=True, help='The base path to save genotype clustermap and other figures')\n",
    "    parser.add_argument('--probe_level', type=int, required=True, help=\"1 for probe level matrix, other numbers for gene_level\")\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config_path = args.config_path\n",
    "    save_path = Path(args.save_path) # Use pathlib\n",
    "    plot_path = Path(args.plot_path) # Use pathlib\n",
    "    probe_level = (args.probe_level == 1)\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    plot_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Process and Filter Individual Datasets ---\n",
    "    processed_hypr_list = []\n",
    "    processed_loom_list = []\n",
    "\n",
    "    print(\"--- Processing Individual Datasets ---\")\n",
    "    for j, (sample_name, meta) in enumerate(datasets.items()):\n",
    "        print(f\"\\nProcessing sample: {sample_name}\")\n",
    "        loom_path = Path(meta['loom_path'])\n",
    "        hypr_path = Path(meta['hypr_path'])\n",
    "\n",
    "        if not loom_path.is_file() or not hypr_path.is_file():\n",
    "            print(f\"  Warning: Input file(s) not found for {sample_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read data\n",
    "            adata_loom_orig = read_tapestri_loom(loom_path)\n",
    "            adata_hypr_orig = sc.read(hypr_path)\n",
    "            print(f\"  Read loom: {adata_loom_orig.shape}\")\n",
    "            print(f\"  Read hypr: {adata_hypr_orig.shape}\")\n",
    "\n",
    "            # Filter hypr\n",
    "            adata_hypr_filtered = filter_hypr(adata_hypr_orig, probe_level=probe_level)\n",
    "            print(f\"  Filtered hypr: {adata_hypr_filtered.shape}\")\n",
    "\n",
    "            # Filter and intersect\n",
    "            adata_hypr_intersect, adata_loom_intersect = find_intersecting_and_filter(adata_hypr_filtered, adata_loom_orig)\n",
    "\n",
    "\n",
    "            # further intersecting the barcodes with the anders' version\n",
    "            # anders_hypr = sc.read(\"/mnt/data/project/25_04_29_Figure3_reanalysis/processed_data/merged_anders/Annotated_phenotype_hypr_seq_MERGED.h5ad\")\n",
    "            # obs_names_of_interest = [obs.split(\"-\")[0] for obs in anders_hypr.obs_names]\n",
    "            # # filter the hypr adata to only keep the barcodes that are in the anders_hypr\n",
    "            # adata_hypr_intersect = adata_hypr_intersect[adata_hypr_intersect.obs_names.isin(obs_names_of_interest), :].copy()\n",
    "            # adata_loom_intersect = adata_loom_intersect[adata_hypr_intersect.obs_names, :].copy()\n",
    "            \n",
    "            if adata_hypr_intersect.shape[0] == 0 or adata_loom_intersect.shape[0] == 0:\n",
    "                 print(f\"  Skipping {sample_name} due to zero intersecting cells after filtering.\")\n",
    "                 continue\n",
    "\n",
    "            print(f\"  Intersected hypr: {adata_hypr_intersect.shape}\")\n",
    "            print(f\"  Intersected loom: {adata_loom_intersect.shape}\")\n",
    "\n",
    "            # Add metadata BEFORE concatenation\n",
    "            adata_hypr_intersect.obs['condition'] = meta['condition']\n",
    "            adata_hypr_intersect.obs['batch'] = meta['batch']\n",
    "            adata_hypr_intersect.obs['sample_name'] = sample_name # Keep original sample name\n",
    "\n",
    "            adata_loom_intersect.obs['condition'] = meta['condition']\n",
    "            adata_loom_intersect.obs['batch'] = meta['batch']\n",
    "            adata_loom_intersect.obs['sample_name'] = sample_name # Keep original sample name\n",
    "\n",
    "            # Append to lists for merging\n",
    "            processed_hypr_list.append(adata_hypr_intersect)\n",
    "            processed_loom_list.append(adata_loom_intersect)\n",
    "            print(f\"  Successfully processed and filtered {sample_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing sample {sample_name}: {e}\")\n",
    "\n",
    "    # --- Merge Datasets ---\n",
    "    if not processed_hypr_list or not processed_loom_list:\n",
    "        print(\"\\nError: No datasets successfully processed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Merging Processed Datasets ---\")\n",
    "    try:\n",
    "        # Concatenate hypr AnnDatas\n",
    "        adata_hypr_merged = ad.concat(\n",
    "            processed_hypr_list,\n",
    "            join='outer',  # Keep all genes/probes, fill missing with 0 or NaN\n",
    "            label='sample_key', # Column name to store original list index (0, 1, 2...)\n",
    "            index_unique='-', # Make cell barcodes unique (e.g., sample_name-barcode)\n",
    "            merge='unique' # How to merge .var/.obs data (use 'unique' or 'same' if appropriate)\n",
    "        )\n",
    "        # Optional: Refill sample_name based on prefix if index_unique was used effectively\n",
    "        # adata_hypr_merged.obs['sample_name'] = [idx.split('-')[0] for idx in adata_hypr_merged.obs_names]\n",
    "        print(f\"Merged hypr data shape: {adata_hypr_merged.shape}\")\n",
    "        print(f\"Merged hypr obs columns: {adata_hypr_merged.obs.columns.tolist()}\")\n",
    "\n",
    "        # Concatenate loom AnnDatas\n",
    "        adata_loom_merged = ad.concat(\n",
    "            processed_loom_list,\n",
    "            join='outer', # Keep all mutations/sites\n",
    "            label='sample_key',\n",
    "            index_unique='-',\n",
    "            merge='unique'\n",
    "        )\n",
    "        # Ensure loom data is aligned to merged hypr data\n",
    "        adata_loom_merged = adata_loom_merged[adata_hypr_merged.obs_names, :].copy()\n",
    "        print(f\"Merged loom data shape: {adata_loom_merged.shape}\")\n",
    "        print(f\"Merged loom obs columns: {adata_loom_merged.obs.columns.tolist()}\")\n",
    "\n",
    "        # Check alignment\n",
    "        if not all(adata_hypr_merged.obs_names == adata_loom_merged.obs_names):\n",
    "             raise ValueError(\"Merged loom and hypr AnnData objects are not perfectly aligned by cell barcode.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during AnnData concatenation: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Annotate Merged Data ---\n",
    "    print(\"\\n--- Annotating Merged Data ---\")\n",
    "    try:\n",
    "        # Annotate the merged data using the genotype info from merged loom\n",
    "        # The annotate_genotype function needs to handle merged data correctly\n",
    "        adata_hypr_annotated, adata_loom_annotated = annotate_genotype(\n",
    "            adata_loom=adata_loom_merged,\n",
    "            adata_hypr=adata_hypr_merged,\n",
    "            config_path=config_path,\n",
    "            save_path=str(save_path), # Pass as string\n",
    "            plot_path=str(plot_path)  # Pass as string\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during annotation: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Save Final Annotated Data ---\n",
    "    print(\"\\n--- Saving Final Annotated Data ---\")\n",
    "    try:\n",
    "        if probe_level:\n",
    "            hypr_name = save_path / \"Annotated_phenotype_hypr_seq_probe_MERGED.h5ad\"\n",
    "        else:\n",
    "            hypr_name = save_path / \"Annotated_phenotype_hypr_seq_MERGED.h5ad\"\n",
    "        loom_name = save_path / \"Annotated_genotype_loom_MERGED.h5ad\"\n",
    "\n",
    "        print(f\"Saving annotated hypr data to: {hypr_name}\")\n",
    "        sc.write(hypr_name, adata_hypr_annotated)\n",
    "\n",
    "        adata_loom_annotated.X = sp.csr_matrix(adata_loom_annotated.X)  # Ensure sparse matrix format\n",
    "        print(f\"Saving annotated loom data to: {loom_name}\")\n",
    "        sc.write(loom_name, adata_loom_annotated)\n",
    "\n",
    "        print(\"Successfully saved final annotated files.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final files: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # default config path is ./variant_config.ini\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76a58e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Next, assign donors\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import issparse, csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "# adata_dna_file = 'path/to/your/dna_anndata.h5ad' # Path to your DNA AnnData file\n",
    "# OR, if adata_dna is already in memory, you can pass it directly to a wrapper function.\n",
    "\n",
    "# Parameters for germline variant selection\n",
    "min_cell_frequency_for_germline = 0.25  # Variant must be non-zero in at least 25% of cells\n",
    "\n",
    "# Clustering parameters\n",
    "n_donors_to_cluster = 2\n",
    "n_pcs_for_clustering = 10 # Number of PCs to use for K-Means (adjust based on variance)\n",
    "kmeans_random_state = 42  # For reproducibility\n",
    "\n",
    "# Output column name for donor labels\n",
    "donor_cluster_col = 'inferred_donor_cluster'\n",
    "\n",
    "def cluster_donors_from_genotypes(adata_dna_input):\n",
    "    \"\"\"\n",
    "    Infers donor clusters from a genotype AnnData object.\n",
    "\n",
    "    Args:\n",
    "        adata_dna_input (sc.AnnData): AnnData object with .X as genotype matrix (cells x variants).\n",
    "                                      Values are expected to be 0 (WT), 1 (Het), 2 (Hom), 3 (Unknown).\n",
    "\n",
    "    Returns:\n",
    "        sc.AnnData: The input AnnData object with a new column in .obs\n",
    "                    (donor_cluster_col) indicating the inferred donor cluster.\n",
    "                    Returns None if critical errors occur.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Donor Clustering by Frequent Variants ---\")\n",
    "    \n",
    "    # Work on a copy to avoid modifying the original object passed to the function\n",
    "    adata_dna = adata_dna_input.copy()\n",
    "\n",
    "    # --- 1. Preprocess Genotype Matrix (adata_dna.X) ---\n",
    "    print(\"\\nStep 1: Preprocessing genotype matrix...\")\n",
    "    \n",
    "    # Ensure .X is not empty\n",
    "    if adata_dna.X is None or adata_dna.shape[0] == 0 or adata_dna.shape[1] == 0:\n",
    "        print(\"Error: adata_dna.X is empty or has zero dimensions.\")\n",
    "        return None\n",
    "\n",
    "    # Handle sparse vs. dense matrix for filling NaNs and replacing 3s\n",
    "    if issparse(adata_dna.X):\n",
    "        print(\"  Matrix is sparse. Converting to CSR for modification.\")\n",
    "        adata_dna.X = adata_dna.X.tocsr()\n",
    "        \n",
    "        # Fill NaNs with 0 (sparse matrices don't explicitly store NaNs in data, but if loaded from dense with NaNs)\n",
    "        # This step is more relevant if X was dense and had NaNs before becoming sparse.\n",
    "        # If X is already sparse and came from a source without NaNs, this might not change much.\n",
    "        # A more robust way for sparse is to ensure no NaNs in data array if they exist\n",
    "        if hasattr(adata_dna.X, 'data') and np.isnan(adata_dna.X.data).any():\n",
    "            print(\"  Filling NaNs in sparse matrix data array with 0...\")\n",
    "            adata_dna.X.data[np.isnan(adata_dna.X.data)] = 0\n",
    "            adata_dna.X.eliminate_zeros() # Clean up explicit zeros\n",
    "\n",
    "        # Change 3s to 0s\n",
    "        print(\"  Changing 3s (Unknown) to 0s (WT) in sparse matrix...\")\n",
    "        adata_dna.X.data[adata_dna.X.data == 3] = 0\n",
    "        adata_dna.X.eliminate_zeros() # Clean up after modification\n",
    "    else: # Dense matrix\n",
    "        print(\"  Matrix is dense.\")\n",
    "        print(\"  Filling NaNs with 0...\")\n",
    "        adata_dna.X = np.nan_to_num(adata_dna.X, nan=0.0)\n",
    "        \n",
    "        print(\"  Changing 3s (Unknown) to 0s (WT)...\")\n",
    "        adata_dna.X[adata_dna.X == 3] = 0\n",
    "    \n",
    "    print(\"  Preprocessing of .X complete.\")\n",
    "\n",
    "    # --- 2. Identify \"Germline-like\" Variants (Frequent Variants) ---\n",
    "    print(\"\\nStep 2: Identifying frequent variants (potential germline markers)...\")\n",
    "    \n",
    "    # Calculate the fraction of cells where each variant is non-zero (i.e., Het or Hom)\n",
    "    if issparse(adata_dna.X):\n",
    "        # For sparse, count non-zero elements per column (variant)\n",
    "        variant_counts_non_zero = adata_dna.X.getnnz(axis=0) # Number of non-zero entries per variant\n",
    "        variant_frequency = variant_counts_non_zero / adata_dna.n_obs\n",
    "    else: # Dense matrix\n",
    "        variant_frequency = np.mean(adata_dna.X != 0, axis=0)\n",
    "        \n",
    "    frequent_variant_mask = variant_frequency >= min_cell_frequency_for_germline\n",
    "    frequent_variant_names = adata_dna.var_names[frequent_variant_mask].tolist()\n",
    "\n",
    "    if not frequent_variant_names:\n",
    "        print(f\"Error: No variants found appearing in > {min_cell_frequency_for_germline*100}% of cells. Cannot proceed.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(frequent_variant_names)} frequent variants (out of {adata_dna.n_vars}).\")\n",
    "\n",
    "    # --- 3. Subset AnnData to Frequent Variants ---\n",
    "    print(\"\\nStep 3: Subsetting AnnData to frequent variants...\")\n",
    "    adata_frequent_vars = adata_dna[:, frequent_variant_names].copy()\n",
    "    print(f\"  Subsetted data shape: {adata_frequent_vars.shape}\")\n",
    "\n",
    "    # --- 4. Clustering ---\n",
    "    print(\"\\nStep 4: Performing PCA and K-Means clustering...\")\n",
    "    \n",
    "    # Ensure data is suitable for PCA (e.g., convert to dense if sparse and PCA needs it)\n",
    "    # Scanpy's PCA can handle sparse data, but let's ensure it's float for stability.\n",
    "    if issparse(adata_frequent_vars.X):\n",
    "        X_for_pca = adata_frequent_vars.X.astype(np.float32)\n",
    "    else:\n",
    "        X_for_pca = adata_frequent_vars.X.astype(np.float32)\n",
    "\n",
    "    # Create a temporary AnnData for PCA if X_for_pca is just the array\n",
    "    # Or, if adata_frequent_vars.X was modified, it can be used directly.\n",
    "    # For simplicity, let's use the .X directly from adata_frequent_vars\n",
    "    \n",
    "    # PCA\n",
    "    # Adjust n_comps: cannot be more than min(n_obs, n_vars)\n",
    "    actual_n_pcs = min(n_pcs_for_clustering, adata_frequent_vars.n_obs -1, adata_frequent_vars.n_vars -1)\n",
    "    if actual_n_pcs < 2: # K-Means on 1D is possible but less common for this type of problem\n",
    "        print(f\"Warning: Number of PCs for clustering is {actual_n_pcs}. Clustering might be suboptimal.\")\n",
    "        if actual_n_pcs <=0:\n",
    "             print(\"Error: Cannot perform PCA with <=0 components. Check data dimensions and n_pcs_for_clustering.\")\n",
    "             return None\n",
    "\n",
    "\n",
    "    print(f\"  Running PCA with {actual_n_pcs} components...\")\n",
    "    try:\n",
    "        sc.tl.pca(adata_frequent_vars, n_comps=actual_n_pcs, svd_solver='arpack', zero_center=True) # zero_center is default True for dense\n",
    "    except Exception as e_pca:\n",
    "        print(f\"  Error during PCA: {e_pca}. Attempting with zero_center=None for sparse if applicable.\")\n",
    "        try:\n",
    "            sc.tl.pca(adata_frequent_vars, n_comps=actual_n_pcs, svd_solver='arpack', zero_center=None if issparse(adata_frequent_vars.X) else True)\n",
    "        except Exception as e_pca2:\n",
    "            print(f\"  PCA failed again: {e_pca2}. Cannot proceed with clustering.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # K-Means Clustering on PCA results\n",
    "    print(f\"  Running K-Means clustering (k={n_donors_to_cluster}) on PCA results...\")\n",
    "    pca_coordinates = adata_frequent_vars.obsm['X_pca']\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_donors_to_cluster, random_state=kmeans_random_state, n_init=10) # n_init='auto' in newer sklearn\n",
    "    cluster_labels = kmeans.fit_predict(pca_coordinates)\n",
    "    \n",
    "    # Store cluster labels in the subsetted AnnData\n",
    "    adata_frequent_vars.obs[donor_cluster_col] = pd.Categorical(cluster_labels.astype(str))\n",
    "    print(\"  Clustering complete.\")\n",
    "    print(f\"  Cluster distribution:\\n{adata_frequent_vars.obs[donor_cluster_col].value_counts()}\")\n",
    "\n",
    "\n",
    "    sc.pp.neighbors(adata_frequent_vars, n_pcs=min(10, adata_frequent_vars.obsm['X_pca'].shape[1]), use_rep='X_pca')\n",
    "    sc.tl.umap(adata_frequent_vars)\n",
    "    sc.pl.umap(adata_frequent_vars, color=donor_cluster_col, title=\"UMAP of Frequent Variants (Sparse) by Inferred Donor\")\n",
    "    plt.savefig(\"plots/umap_frequent_variants_sparse.png\", dpi=300)\n",
    "    # --- 5. Add Cluster Labels to Original AnnData ---\n",
    "    # We add it to the copy we made at the beginning (adata_dna)\n",
    "    # or to the input object directly if preferred (adata_dna_input)\n",
    "    print(f\"\\nStep 5: Adding '{donor_cluster_col}' to the input AnnData object's .obs...\")\n",
    "    adata_dna_input.obs[donor_cluster_col] = adata_frequent_vars.obs[donor_cluster_col].reindex(adata_dna_input.obs_names)\n",
    "    \n",
    "    print(\"--- Donor Clustering Complete ---\")\n",
    "    return adata_dna_input\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    adata_loom = sc.read(\"/mnt/data/project/25_04_29_Figure3_reanalysis/processed_data/merged_v9/Annotated_genotype_loom_MERGED.h5ad\")\n",
    "    adata_dna_dummy_sparse_clustered = cluster_donors_from_genotypes(adata_loom)\n",
    "\n",
    "    adata_rna = sc.read(\"/mnt/data/project/25_04_29_Figure3_reanalysis/processed_data/merged_v9/Annotated_phenotype_hypr_seq_MERGED.h5ad\")\n",
    "    adata_rna.obs['inferred_donor_cluster'] = adata_dna_dummy_sparse_clustered.obs['inferred_donor_cluster'].reindex(adata_rna.obs_names)\n",
    "    adata_rna.write(\"/mnt/data/project/25_04_29_Figure3_reanalysis/processed_data/merged_v9/Annotated_phenotype_hypr_seq_MERGED_with_donor_clusters.h5ad\")\n",
    "    if adata_dna_dummy_sparse_clustered is not None:\n",
    "        print(\"\\nClustering results on SPARSE dummy data (head of .obs):\")\n",
    "        print(adata_dna_dummy_sparse_clustered.obs.head())\n",
    "        print(\"\\nCluster counts:\")\n",
    "        print(adata_dna_dummy_sparse_clustered.obs[donor_cluster_col].value_counts())\n",
    "\n",
    "        # Optional: Visualize UMAP of frequent variants colored by cluster\n",
    "        # Re-calculate frequent_variant_mask based on the potentially modified sparse matrix\n",
    "        if issparse(adata_dna_dummy_sparse_clustered.X):\n",
    "            variant_counts_non_zero_s = adata_dna_dummy_sparse_clustered.X.getnnz(axis=0)\n",
    "            variant_frequency_s = variant_counts_non_zero_s / adata_dna_dummy_sparse_clustered.n_obs\n",
    "        else:\n",
    "            variant_frequency_s = np.mean(adata_dna_dummy_sparse_clustered.X != 0, axis=0)\n",
    "        frequent_variant_mask_s = variant_frequency_s >= min_cell_frequency_for_germline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4f654",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Then, compute DEGs\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "adata_file = '/mnt/data/project/25_04_29_Figure3_reanalysis/processed_data/merged_v9/Annotated_phenotype_hypr_seq_MERGED_with_donor_clusters.h5ad'\n",
    "# geno = sc.read(\"/mnt/data/project/25_04_29_Figure3_reanalysis/data_anders/tapestri_adata.h5ad\")\n",
    "genotype_col = 'genotype_annotation'\n",
    "condition_col = 'condition'\n",
    "wt_genotype_label = 'AAV_control' # Genotype to compare against\n",
    "\n",
    "# DE Analysis Parameters\n",
    "de_method = 'wilcoxon'\n",
    "de_corr_method = 'benjamini-hochberg'\n",
    "# No filtering thresholds needed here as we save all genes\n",
    "\n",
    "# Output directory and file\n",
    "output_dir = Path(f'./analysis_results_v9/deg_vs_{wt_genotype_label}_wide_table') # Updated output dir name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_filename = output_dir / f\"all_pairwise_deg_stats_vs_{wt_genotype_label}_wide.csv\"\n",
    "# --- 1. Load Data and Prepare ---\n",
    "print(f\"Loading full annotated data from: {adata_file}\")\n",
    "try:\n",
    "    if isinstance(adata_file, (str, Path)):\n",
    "        adata_full = sc.read_h5ad(adata_file)\n",
    "        # adata_full.obs['inferred_donor_cluster'] = geno.obs['genotype_cluster']\n",
    "        # sc.pp.filter_cells(adata_full, min_counts=500)  # Filter genes with <3 cells\n",
    "    elif isinstance(adata_file, sc.AnnData):\n",
    "        adata_full = adata_file # Use the object directly if passed\n",
    "    else:\n",
    "        raise TypeError(\"adata_file must be a path string or an AnnData object.\")\n",
    "    print(\"Full data loaded successfully.\")\n",
    "\n",
    "    required_obs_cols = [genotype_col, condition_col]\n",
    "    for col in required_obs_cols:\n",
    "        if col not in adata_full.obs.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in adata_full.obs.\")\n",
    "\n",
    "    # Prepare expression data for DE (log-normalized)\n",
    "    if 'log1p' in adata_full.uns and adata_full.X is not None and hasattr(adata_full.X, 'expm1'): # Check if log1p was likely applied\n",
    "        adata_processed = adata_full.copy()\n",
    "        print(\"Using expression from adata_full.X (assumed log-normalized).\")\n",
    "    elif adata_full.X is not None:\n",
    "        print(\"Using adata_full.X for DE processing (will normalize and log1p if needed)...\")\n",
    "        adata_processed = adata_full.copy()\n",
    "        sc.pp.normalize_total(adata_processed, target_sum=1e4)\n",
    "        sc.pp.log1p(adata_processed)\n",
    "        print(\"  Normalization and log1p applied to .X data if needed.\")\n",
    "    else:\n",
    "        raise ValueError(\"Suitable expression data not found in .X or .raw for processing.\")\n",
    "    \n",
    "    adata_processed.obs[genotype_col] = adata_processed.obs[genotype_col].astype('category')\n",
    "    adata_processed.obs[condition_col] = adata_processed.obs[condition_col].astype('category')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AnnData file not found at {adata_file}.\")\n",
    "    exit()\n",
    "except (ValueError, TypeError) as ve:\n",
    "     print(f\"Error: {ve}\")\n",
    "     exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading/preparing the AnnData file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Perform Pairwise DE Analysis and Collect Results for Merging ---\n",
    "# compute DE for only donor 0\n",
    "adata_processed = adata_processed[adata_processed.obs['inferred_donor_cluster'] == \"0\"].copy()\n",
    "\n",
    "list_of_de_dataframes_for_merge = []\n",
    "\n",
    "unique_conditions = adata_processed.obs[condition_col].cat.categories.tolist()\n",
    "unique_genotypes = adata_processed.obs[genotype_col].cat.categories.tolist()\n",
    "print(f\"\\nFound conditions: {unique_conditions}\")\n",
    "print(f\"Found genotypes: {unique_genotypes}\")\n",
    "\n",
    "for current_cond in unique_conditions:\n",
    "    print(f\"\\n--- Processing Condition: {current_cond} ---\")\n",
    "    adata_cond = adata_processed[adata_processed.obs[condition_col] == current_cond].copy()\n",
    "    \n",
    "    if wt_genotype_label not in adata_cond.obs[genotype_col].cat.categories:\n",
    "        print(f\"  Warning: WT genotype '{wt_genotype_label}' not found in condition '{current_cond}'. Skipping DE for this condition.\")\n",
    "        continue\n",
    "\n",
    "    for current_geno in unique_genotypes:\n",
    "        if current_geno == wt_genotype_label:\n",
    "            continue \n",
    "\n",
    "        print(f\"  Comparing: '{current_geno}' vs '{wt_genotype_label}'\")\n",
    "        groups_to_compare = [current_geno, wt_genotype_label]\n",
    "        adata_comparison = adata_cond[adata_cond.obs[genotype_col].isin(groups_to_compare)].copy()\n",
    "        \n",
    "        group_counts = adata_comparison.obs[genotype_col].value_counts()\n",
    "        if len(group_counts) < 2 or group_counts.get(current_geno, 0) < 3 or group_counts.get(wt_genotype_label, 0) < 3:\n",
    "            print(f\"    Skipping: Insufficient cells or groups for DE. Counts: {group_counts.to_dict()}\")\n",
    "            continue\n",
    "            \n",
    "        adata_comparison.obs[genotype_col] = adata_comparison.obs[genotype_col].astype('category').cat.set_categories(groups_to_compare)\n",
    "\n",
    "        try:\n",
    "            sc.tl.rank_genes_groups(\n",
    "                adata_comparison,\n",
    "                groupby=genotype_col,\n",
    "                groups=[current_geno], \n",
    "                reference=wt_genotype_label,\n",
    "                method=de_method,\n",
    "                use_raw=False, \n",
    "                corr_method=de_corr_method,\n",
    "                n_genes=adata_comparison.n_vars \n",
    "            )\n",
    "            \n",
    "            degs_df_full = sc.get.rank_genes_groups_df(adata_comparison, group=current_geno)\n",
    "            \n",
    "            # Prepare DataFrame for merging: index=gene_names, specific columns for logFC and pval_adj\n",
    "            df_for_merge = degs_df_full[['names', 'logfoldchanges', 'pvals_adj']].copy()\n",
    "            df_for_merge = df_for_merge.set_index('names')\n",
    "            \n",
    "            # Create specific column names\n",
    "            # Sanitize current_geno for column names (replace special characters)\n",
    "            sanitized_geno_name = re.sub(r'[^A-Za-z0-9_]+', '_', current_geno)\n",
    "            \n",
    "            logfc_col_name = f\"{current_cond}_{sanitized_geno_name}_vs_{wt_genotype_label}_logFC\"\n",
    "            pval_adj_col_name = f\"{current_cond}_{sanitized_geno_name}_vs_{wt_genotype_label}_pval_adj\"\n",
    "            \n",
    "            df_for_merge = df_for_merge.rename(columns={\n",
    "                'logfoldchanges': logfc_col_name,\n",
    "                'pvals_adj': pval_adj_col_name\n",
    "            })\n",
    "            \n",
    "            list_of_de_dataframes_for_merge.append(df_for_merge)\n",
    "            \n",
    "            print(f\"    Processed DE for {len(df_for_merge)} genes. Stats will be added to the wide table.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error during DE analysis for '{current_geno}' vs '{wt_genotype_label}' in condition '{current_cond}': {e}\")\n",
    "\n",
    "# --- 3. Merge all DE results into a single wide DataFrame ---\n",
    "print(\"\\n--- Merging all DE results into a single wide table ---\")\n",
    "if list_of_de_dataframes_for_merge:\n",
    "    # Concatenate along axis=1 (columns), joining on the gene index\n",
    "    # 'outer' join ensures all genes from all comparisons are kept\n",
    "    final_wide_df = pd.concat(list_of_de_dataframes_for_merge, axis=1, join='outer')\n",
    "    final_wide_df = final_wide_df.sort_index() # Sort by gene name\n",
    "    \n",
    "    print(f\"Final wide DataFrame shape: {final_wide_df.shape}\")\n",
    "    print(\"Head of the final wide DataFrame:\")\n",
    "    print(final_wide_df.head())\n",
    "\n",
    "    # Save the wide DataFrame\n",
    "    try:\n",
    "        final_wide_df.to_csv(output_filename)\n",
    "        print(f\"  Successfully saved wide DEG table to: {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving wide DEG table: {e}\")\n",
    "else:\n",
    "    print(\"No DE results were generated to merge and save.\")\n",
    "\n",
    "print(\"\\n--- Pairwise DE Analysis (Single Wide Table) Complete ---\")\n",
    "\n",
    "# save the genotype counts per condition in the output folder\n",
    "# we have 2 conditions and many genotypes, so we will count how many cells are in each genotype per condition\n",
    "genotype_counts = adata_processed.obs.groupby([condition_col, genotype_col]).size().unstack(fill_value=0)\n",
    "genotype_counts_filename = output_dir / \"genotype_counts_per_condition.csv\"\n",
    "try:\n",
    "    genotype_counts.to_csv(genotype_counts_filename)\n",
    "    print(f\"  Successfully saved genotype counts per condition to: {genotype_counts_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error saving genotype counts per condition: {e}\")\n",
    "# --- End of Script ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
